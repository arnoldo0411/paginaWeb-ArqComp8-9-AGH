<html>

<head>
<title>Pagina web Arquitectura Comp </title>
</head>

<body background = "PaginaWeb/imagenes/fondo5.jpeg">

<center><h1><font color = "white" >* ARQUITECTURA DE COMPUTADORAS *</font></h1></center>
<img src = "PaginaWeb/imagenes/logo.jpg">
<center>

<a href = "PaginaW.html">
<img src = "PaginaWeb/imagenes/inicio.png"></a>

<a href = "uni1.html">
<img src = "PaginaWeb/imagenes/uni1.png"></a>

<a href = "uni2.html">
<img src = "PaginaWeb/imagenes/uni2.png"></a>

<a href = "uni3.html">
<img src = "PaginaWeb/imagenes/uni3.png"></a>

<a href = "uni4.html">
<img src = "PaginaWeb/imagenes/uni4.png"></a>
</center>

<center><h1><font color = "red" >*Tema: Ambientes de Servicio*</font></h1></center>
<center><table border = "3"><td>
<center><h2><font color = "white">
<p>El negocio de proveer servicios de datos es mucho más complejo
que la forma en la que se dan los tradicionales servicios, los
primeros requieren de nuevos conocimientos y modelos de negocio,
que con frecuencia se termina involucrando o necesitando la
colaboración de terceras empresas. Por lo que se hace necesario que
los operadores tradicionales transformen su negocio para ofrecer los
servicios de datos con los niveles de servicio que el mercado exige.</p>
<p>- Negocios.</p>
<p>Definitivamente, la tecnología en general ha sido la causa principal
y la acción más directa para la transformación del trabajo de las
organizaciones en la posguerra del siglo XX. Tanto los bienes de
capital "duros" (computadores, teléfonos, videos, facsímiles,
grabadoras, etc.), como los programas y sistemas de información y
comunicación en general, han incrementado enormemente la
productividad y eficiencia de las organizaciones. Tenemos como
ejemplos los siguientes: bases de datos en redes de todo orden y
topología, sistemas de reservaciones en aerolíneas, sistemas de
contabilidad y nóminas, archivos clínicos en centros de salud,
sistemas de conmutación electrónica y un sin número de otras
aplicaciones a procesos administrativos.</p>
<p>- Industria</p>
<p>La industrialización de los servicios de tecnología de información va
a redefinir el mercado en términos de como las organizaciones
evalúan, compran y seleccionan los servicios y como los vendedores
desarrollan y establecen precios de los servicios.</p>

<p>Para lograr esta estandarización, se requiere un enfoque hacia las
soluciones genéricas y esto debe ser responsabilidad de los
proveedores, que deben de desarrollar, operar y administrar el
resultado de estos genéricos de TI.
Aunque los servicios de TI están en proceso de madurez, la madurez
de la industria se ha incrementado en aspectos evidentes, como la
forma en que los servicios son implementados y administrados.</p>

<p>- Comercio Electrónico.</p>
<p>El desarrollo de estas tecnologías y de las telecomunicaciones ha
hecho que los intercambios de datos crezcan a niveles
extraordinarios, simplificándose cada vez más y creando nuevas
formas de comercio, y en este marco se desarrolla el Comercio
Electrónico.</p>
<p>Se considera “Comercio Electrónico” al conjunto de aquellas
transacciones comerciales y financieras realizadas a través del
procesamiento y la transmisión de información, incluyendo texto,
sonido e imagen.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u61.jpg"></center>

</table></center>

<center><h1><font color = "red" >*Tema: Procesamiento Paralelo*</font></h1></center>
<center><table border = "3"><td>
<center><h2><font color = "white">
<p>- Aspectos Básicos de la computación paralela.</p>
<p>La computación paralela es una forma de cómputo en la que muchas
instrucciones se ejecutan simultáneamente, operando sobre el
principio de que problemas grandes, a menudo se pueden dividir en
unos más pequeños, que luego son resueltos simultáneamente (en
paralelo). Hay varias formas diferentes de computación paralela:
paralelismo a nivel de bit, paralelismo a nivel de instrucción,
paralelismo de datos y paralelismo de tareas. El paralelismo se ha
empleado durante muchos años, sobre todo en la computación de
altas prestaciones, pero el interés en ella ha crecido últimamente
debido a las limitaciones físicas que impiden el aumento de la
frecuencia. Como el consumo de energía y por consiguiente la
generación de calor de las computadoras constituye una
preocupación en los últimos años, la computación en paralelo se ha
convertido en el paradigma dominante en la arquitectura de
computadores, principalmente en forma de procesadores
multinúcleo.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u62.jpg"></center>

<center><h2><font color = "white">
<p>- Ley de Amdahl y ley de Gustafson.</p>
<p>Idealmente, la aceleración a partir de la paralelización es lineal,
doblar el número de elementos de procesamiento debe reducir a la
mitad el tiempo de ejecución y doblarlo por segunda vez debe
nuevamente reducir el tiempo a la mitad. Sin embargo, muy pocos
algoritmos paralelos logran una aceleración óptima. La mayoría
tienen una aceleración casi lineal para un pequeño número de
elementos de procesamiento, y pasa a ser constante para un gran
número de elementos de procesamiento.</p>
<p<La aceleración potencial de un algoritmo en una plataforma de
cómputo en paralelo está dada por la ley de Amdahl, formulada
originalmente por Gene Amdahl en la década de 1960. Esta señala
que una pequeña porción del programa que no pueda paralelizarse
va a limitar la aceleración que se logra con la paralelización. Los
programas que resuelven problemas matemáticos o ingenieriles
típicamente consisten en varias partes paralelizables y varias no
paralelizables (secuenciales).</p>

<p>La ley de Gustafson es otra ley en computación que está en estrecha
relación con la ley de Amdahl. Ambas leyes asumen que el tiempo
de funcionamiento de la parte secuencial del programa es
independiente del número de procesadores. La ley de Amdahl
supone que todo el problema es de tamaño fijo, por lo que la
cantidad total de trabajo que se hará en paralelo también es
independiente del número de procesadores, mientras que la ley de
Gustafson supone que la cantidad total de trabajo que se hará en
paralelo varía linealmente con el número de procesadores.
Dependencias.</p>
<p>Entender la dependencia de datos es fundamental en la
implementación de algoritmos paralelos. Ningún programa puede
ejecutar más rápidamente que la cadena más larga de cálculos
dependientes (conocida como la ruta crítica), ya que los cálculos que
dependen de cálculos previos en la cadena deben ejecutarse en
orden. Sin embargo, la mayoría de los algoritmos no consisten sólo
de una larga cadena de cálculos dependientes; generalmente hay
oportunidades para ejecutar cálculos independientes en paralelo.
Sea Pi y Pj dos segmentos del programa. Las condiciones de
Bernstein describen cuando los dos segmentos son independientes y
pueden ejecutarse en paralelo. Para Pi, sean Ii todas las variables de
entrada y Oi las variables de salida, y del mismo modo para Pj. Pi y
Pj son independientes si satisfacen.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u63.jpg"></center>

<center><h2><font color = "white">
<p>- Condiciones de carrera, exclusión mutua, sincronización, y
desaceleración paralela.</p>
<p>Las subtareas en un programa paralelo a menudo son llamadas hilos.
Algunas arquitecturas de computación paralela utilizan versiones
más pequeñas y ligeras de hilos conocidas como hebras, mientras
que otros utilizan versiones más grandes conocidos como procesos.
Sin embargo, «hilos» es generalmente aceptado como un término
genérico para las subtareas. Los hilos a menudo tendrán que
actualizar algunas variables que se comparten entre ellos. Las
instrucciones entre los dos programas pueden entrelazarse en
cualquier orden.</p>
<p>Las aplicaciones a menudo se clasifican según la frecuencia con que
sus subtareas se sincronizan o comunican entre sí. Una aplicación
muestra un paralelismo de grano fino si sus subtareas deben
comunicase muchas veces por segundo, se considera paralelismo de
grano grueso si no se comunican muchas veces por segundo, y es
vergonzosamente paralelo si nunca o casi nunca se tienen que
comunicar.</p>
<p>-Aplicaciones vergonzosamente paralelas son consideradas las más
fáciles de paralelizar.
Grano de paralelismo.
Muy grueso: Programas.
Grueso: Subprogramas, tareas.
Fino: Instrucción.
Muy fino: Fases de instrucción.</p>

<p>- Modelos de consistencia.</p>
<p>Los lenguajes de programación en paralelo y computadoras paralelas
deben tener un modelo de consistencia de datos también conocido
como un modelo de memoria.</p>
<p>El modelo de consistencia define reglas para las operaciones en la
memoria del ordenador y cómo se producen los resultados.
Uno de los primeros modelos de consistencia fue el modelo de
consistencia secuencial de Leslie Lamport. La consistencia
secuencial es la propiedad de un programa en la que su ejecución en
paralelo produce los mismos resultados que un programa secuencial.</p<
<p>Específicamente, es un programa secuencial consistente si "... los
resultados de una ejecución son los mismos que se obtienen si las
operaciones de todos los procesadores son ejecutadas en un orden
secuencial, y las operaciones de cada procesador individual aparecen
en esta secuencia en el orden especificado por el programa".</p>
</font></h2></center>

<center><h2><font color = "white">
<p>- Taxonomía de Flynn.</p>
<p>* Single Instruction, Single Data (SISD).</p>
<p>Hay un elemento de procesamiento, que tiene acceso a un único
programa y a un almacenamiento de datos. En cada paso, el
elemento de procesamiento carga una instrucción y la información
correspondiente y ejecuta esta instrucción. El resultado es guardado
de vuelta en el almacenamiento de datos. Luego SISD es el
computador secuencial convencional, de acuerdo al modelo de von
Neumann.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u64.jpg"></center>

<center><h2><font color = "white">
<p>- Multiple Instruction, Single Data (MISD).</p>
<p>Hay múltiples elementos de procesamiento, en el que cada cual tiene
memoria privada del programa, pero se tiene acceso común a una
memoria global de información. En cada paso, cada elemento de
procesamiento de obtiene la misma información de la memoria y
carga una instrucción de la memoria privada del programa. Luego,
las instrucciones posiblemente diferentes de cada unidad, son
ejecutadas en paralelo, usando la información (idéntica) recibida
anteriormente. Este modelo es muy restrictivo y no se ha usado en
ningún computador de tipo comercial.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u65.jpg"></center>

<center><h2><font color = "white">
<p>- Single Instruction, Multiple Data (SIMD).</p>
<p>Hay múltiples elementos de procesamiento, en el que cada cual tiene
acceso privado a la memoria de información (compartida o
distribuida). Sin embargo, hay una sola memoria de programa, desde
la cual una unidad de procesamiento especial obtiene y despacha
instrucciones. En cada paso, cada unidad de procesamiento obtiene
la misma instrucción y carga desde su memoria privada un elemento
de información y ejecuta esta instrucción en dicho elemento.</p>
<p>Entonces, la instrucción es síncronamente aplicada en paralelo por
todos los elementos de proceso a diferentes elementos de
información. Para aplicaciones con un grado significante de
paralelismo de información, este acercamiento puede ser muy
eficiente. Ejemplos pueden ser aplicaciones multimedia y algoritmos
de gráficos de computadora.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u66.jpg"></center>

<center><h2><font color = "white">
<p>- Multiple Instruction, Multiple Data (MIMD).</p>
<p>Hay múltiples unidades de procesamiento, en la cual cada una tiene
tanto instrucciones como información separada. Cada elemento
ejecuta una instrucción distinta en un elemento de información
distinto. Los elementos de proceso trabajan asíncronamente. Los
clusters son ejemplo son ejemplos del modelo MIMD.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u67.jpg"></center>

</table></center>

<center><h1><font color = "red" >*Tema: Tipos de Computacion Paralela.*</font></h1></center>
<center><table border = "3"><td>
<center><h2><font color = "white">
<p>- Paralelismo a nivel de bit.</p>
<p>Desde el advenimiento de la integración a gran escala (VLSI) como
tecnología de fabricación de chips de computadora en la década de
1970 hasta alrededor de 1986, la aceleración en la arquitectura de
computadores se lograba en gran medida duplicando el tamaño de la
palabra en la computadora, la cantidad de información que el
procesador puede manejar por ciclo. El aumento del tamaño de la
palabra reduce el número de instrucciones que el procesador debe
ejecutar para realizar una operación en variables cuyos tamaños son
mayores que la longitud de la palabra. Por ejemplo, cuando un
procesador de 8 bits debe sumar dos enteros de 16 bits, el
procesador primero debe adicionar los 8 bits de orden inferior de
cada número entero con la instrucción de adición, a continuación,
añadir los 8 bits de orden superior utilizando la instrucción de
adición con acarreo que tiene en cuenta el bit de acarreo de la
adición de orden inferior, en este caso un procesador de 8 bits
requiere dos instrucciones para completar una sola operación, en
donde un procesador de 16 bits necesita una sola instrucción para
poder completarla.</p>	
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u68.jpg"></center>

<center><h2><font color = "white">
<p>- Paralelismo a nivel de instrucción.</p>
<p>Los procesadores modernos tienen ''pipeline'' de instrucciones de
varias etapas. Cada etapa en el pipeline corresponde a una acción
diferente que el procesador realiza en la instrucción correspondiente
a la etapa; un procesador con un pipeline de N etapas puede tener
hasta n instrucciones diferentes en diferentes etapas de finalización.
El ejemplo canónico de un procesador segmentado es un procesador
RISC, con cinco etapas: pedir instrucción, decodificar, ejecutar,
acceso a la memoria y escritura. El procesador Pentium 4 tenía un
pipeline de 35 etapas.</p>	
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u69.png"></center>

<center><h2><font color = "white">
<p>- Paralelismo de datos.</p>
<p>El paralelismo de datos es el paralelismo inherente en programas
con ciclos, que se centra en la distribución de los datos entre los
diferentes nodos computacionales que deben tratarse en paralelo.
"La paralelización de ciclos conduce a menudo a secuencias
similares de operaciones —no necesariamente idénticas— o
funciones que se realizan en los elementos de una gran estructura de
datos". Muchas de las aplicaciones científicas y de ingeniería
muestran paralelismo de datos.</p>
<p>Una dependencia de terminación de ciclo es la dependencia de una
iteración de un ciclo en la salida de una o más iteraciones anteriores.
Las dependencias de terminación de ciclo evitan la paralelización de
ciclos.</p>	
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u70.jpg"></center>

<center><h2><font color = "white">
<p>- Paralelismo de tareas.</p>
<p>Paralelismo de tareas es un paradigma de la programación
concurrente que consiste en asignar distintas tareas a cada uno de los
procesadores de un sistema de cómputo. En consecuencia, cada
procesador efectuará su propia secuencia de operaciones.
En su modo más general, el paralelismo de tareas se representa
mediante un grafo de tareas, el cual es subdividido en subgrafos que
son luego asignados a diferentes procesadores. De la forma como se
corte el grafo, depende la eficiencia de paralelismo resultante. La
partición y asignación óptima de un grafo de tareas para ejecución
concurrente es un problema NP-completo, por lo cual en la práctica
se dispone de métodos heurísticos aproximados para lograr una
asignación cercana a la óptima.</p>

<p>Sin embargo, existen ejemplos de paralelismo de tareas restringido
que son de interés en programación concurrente. Tal es el caso del
paralelismo encauzado, en el cual el grafo tiene forma de cadena,
donde cada nodo recibe datos del nodo previo y sus resultados son
enviados al nodo siguiente. El carácter simplificado de este modelo
permite obtener paralelismo de eficiencia óptima.</p>	
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u71.jpg"></center>

<center><h2><font color = "white">
<p>- Clasificación.</p>
<>Las computadoras paralelas se pueden clasificar de acuerdo con el
nivel en el que el hardware soporta paralelismo. Esta clasificación es
análoga a la distancia entre los nodos básicos de cómputo. Estos no
son excluyentes entre sí, por ejemplo, los grupos de
multiprocesadores simétricos son relativamente comunes.</p>
<p>* Computación multinúcleo: un procesador multinúcleo es un
procesador que incluye múltiples unidades de ejecución
(núcleos) en el mismo chip. Un procesador multinúcleo puede
ejecutar múltiples instrucciones por ciclo de secuencias de
instrucciones múltiples.</p>
<p>* Multiprocesamiento simétrico: un multiprocesador simétrico
(SMP) es un sistema computacional con múltiples
procesadores idénticos que comparten memoria y se conectan
a través de un bus. La contención del bus previene el escalado
de esta arquitectura.</p>
<p>* Computación en clúster: un clúster es un grupo de
ordenadores débilmente acoplados que trabajan en estrecha
colaboración, de modo que en algunos aspectos pueden
considerarse como un solo equipo.</p>
<p>* Procesamiento paralelo masivo: tienden a ser más grandes
que los clústeres, con «mucho más» de 100 procesadores. En
un MPP, cada CPU tiene su propia memoria y una copia del
sistema operativo y la aplicación.</p>
<p>* Computación distribuida: la computación distribuida es la
forma más distribuida de la computación paralela. Se hace uso
de ordenadores que se comunican a través de la Internet para
trabajar en un problema dado.</p>

<p>* Computadoras paralelas especializadas: dentro de la
computación paralela, existen dispositivos paralelos
especializados que generan interés. Aunque no son específicos
para un dominio, tienden a ser aplicables sólo a unas pocas
clases de problemas paralelos.</p>
<p>* Cómputo reconfigurable con arreglos de compuertas
programables: el cómputo reconfigurable es el uso de un
arreglo de compuertas programables (FPGA) como
coprocesador de un ordenador de propósito general.</p>
<p>* Cómputo de propósito general en unidades de
procesamiento gráfico (GPGPU): es una tendencia
relativamente reciente en la investigación de ingeniería
informática. Los GPUs son co-procesadores que han sido
fuertemente optimizados para procesamiento de gráficos por
computadora.</p>
<p>* Circuitos integrados de aplicación específica: debido a que
un ASIC (por definición) es específico para una aplicación
dada, puede ser completamente optimizado para esa
aplicación. Como resultado, para una aplicación dada, un
ASIC tiende a superar a un ordenador de propósito general.</p>
<p>* Procesadores vectoriales: pueden ejecutar la misma
instrucción en grandes conjuntos de datos. Tienen operaciones
de alto nivel que trabajan sobre arreglos lineales de números o
vectores.</p>

<p>- Arquitectura de computadores secuenciales.</p>
<p>A diferencia de los sistemas combinacionales, en los sistemas
secuenciales, los valores de las salidas, en un momento dado, no
dependen exclusivamente de los valores de las entradas en dicho
momento, sino también dependen del estado anterior o estado
interno. El sistema secuencial más simple es el biestable, de los
cuales, el de tipo D (o cerrojo) es el más utilizado actualmente.</p>
<p>El sistema secuencial requiere de la utilización de un dispositivo de
memoria que pueda almacenar la historia pasada de sus entradas
(denominadas variables de estado) y le permita mantener su estado
durante algún tiempo, estos dispositivos de memoria pueden ser
sencillos como un simple retardador o celdas de memoria de tipo
DRAM, SRAM o multivibradores biestables también conocido
como Flip-Flop.</p>
<p>- Tipos de sistemas secuenciales.</p>
<p>En este tipo de circuitos entra un factor que no se había considerado
en los circuitos combinacionales, dicho factor es el tiempo, según
como manejan el tiempo se pueden clasificar en: circuitos
secuenciales síncronos y circuitos secuenciales asíncronos.</p>

<p>- Circuitos secuenciales asíncronos.</p>
<p>En circuitos secuenciales asíncronos los cambios de estados ocurren
al ritmo natural asociado a las compuertas lógicas utilizadas en su
implementación, lo que produce retardos en cascadas entre los
biestables del circuito, es decir no utilizan elementos especiales de
memoria, lo que puede ocasionar algunos problemas de
funcionamiento, ya que estos retardos naturales no están bajo el
control del diseñador y además no son idénticos en cada compuerta
lógica.</p>
<p>- Circuitos secuenciales síncronos.</p>
<p>Los circuitos secuenciales síncronos solo permiten un cambio de
estado en los instantes marcados o autorizados por una señal de
sincronismo de tipo oscilatorio denominada reloj (cristal o circuito
capaz de producir una serie de pulsos regulares en el tiempo), lo que
soluciona los problemas que tienen los circuitos asíncronos
originados por cambios de estado no uniformes dentro del sistema o
circuito.</p>
<p>- Organización de direcciones de memoria.</p>
<p>La memoria principal en un ordenador en paralelo puede ser
compartida —compartida entre todos los elementos de
procesamiento en un único espacio de direcciones—, o distribuida
—cada elemento de procesamiento tiene su propio espacio local de
direcciones—. El término memoria distribuida se refiere al hecho de
que la memoria se distribuye lógicamente, pero a menudo implica
que también se distribuyen físicamente. La memoria distribuida-
compartida y la virtualización de memoria combinan los dos
enfoques, donde el procesador tiene su propia memoria local y
permite acceso a la memoria de los procesadores que no son locales.
Los accesos a la memoria local suelen ser más rápidos que los
accesos a memoria no local.</p>

<p>Las arquitecturas de ordenador en las que cada elemento de la
memoria principal se puede acceder con igual latencia y ancho de
banda son conocidas como arquitecturas de acceso uniforme a
memoria (UMA). Típicamente, sólo se puede lograr con un sistema
de memoria compartida, donde la memoria no está distribuida
físicamente. Un sistema que no tiene esta propiedad se conoce como
arquitectura de acceso a memoria no uniforme (NUMA). Los
sistemas de memoria distribuidos tienen acceso no uniforme a la
memoria.</p>	
</font></h2></center>

</table></center>

<center><h1><font color = "red" >*Tema: Sistemas.*</font></h1></center>
<center><table border = "3"><td>
<center><h2><font color = "white">
<p>- Sistemas de memoria compartida (multiprocesadores).</p>
<p>* Todos los procesadores acceden a una memoria común.
* La comunicación entre procesadores se hace a través de la
memoria.
* Se necesitan primitivas de sincronismo para asegurar el
intercambio de datos.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u72.jpg"></center>

<center><h2><font color = "white">
<p>- Estructura de los multiprocesadores de memoria compartida.</p>
<p>La mayoría de los multiprocesadores comerciales son del tipo UMA
(Uniform Memory Access): todos los procesadores tienen igual
tiempo de acceso a la memoria compartida. En la arquitectura UMA
los procesadores se conectan a la memoria a través de un bus, una
red multietapa o un conmutador de barras cruzadas (red multietapa o
un conmutador de barras cruzadas (crossbar crossbar) y disponen de
su propia ) y disponen de su propia memoria caché. Los
procesadores tipo NUMA (Non Uniform Memory Access) presentan
tiempos de acceso a la memoria compartida que dependen de la
ubicación del elemento de proceso y la memoria.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u73.jpg"></center>

<center><h2><font color = "white">
<p>- Redes de interconexión dinámica (indirecta).</p>
<p>*Medio compartido.*</p>
<p>*Conexión por bus compartido.</p>
<p>Es la organización más común en los computadores personales y
servidores.</p>
<p>El bus consta de líneas de dirección, datos y control para
implementar:</p>
<p>* El protocolo de transferencias de datos con la memoria.
* El arbitraje del acceso al bus cuando más de un procesador
compite por utilizarlo.</p>
<p>- Los procesadores utilizan cachés locales para:</p>
<p>* Reducir el tiempo medio de acceso a memoria, como en un
monoprocesador.
* Disminuir la utilización del bus compartido.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u74.jpg"></center>

<center><h2><font color = "white">
<p>- Protocolos de transferencia de ciclo partido.</p>
<p>La operación de lectura se divide en dos transacciones no continuas
de acceso al bus. La primera es de petición de lectura que realiza el
máster (procesador) sobre el slave (memoria). Una vez realizada la
petición el máster abandona el bus. Cuando el slave dispone del dato
leído, inicia un ciclo de bus actuando como máster para enviar el
dato al antiguo máster, que ahora actúa como slave.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u75.jpg"></center>

<center><h2><font color = "white">
<p>- Protocolo de arbitraje distribuido.</p>
<p>La responsabilidad del arbitraje se distribuye por los diferentes
procesadores conectados al bus.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u76.jpg"></center>

<center><h2><font color = "white">
<p>- Conmutadas.</p>
<p>* Conexión por conmutadores crossbar.</p>
<p>Cada procesador (Pi) y cada módulo de memoria (Mi) tienen su
propio bus. Existe un conmutador (S) en los puntos de intersección
que permite conectar un bus de memoria con un bus de procesador.
Para evitar conflictos cuando más de un procesador pretende acceder
al mismo módulo de memoria se establece un orden de prioridad. Se
trata de una red sin bloqueo con una conectividad completa pero de
alta complejidad.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u77.jpg"></center>

<center><h2><font color = "white">
<p>- Conmutadas.</p>
<p>Conexión por conmutadores crossbar.</p>
<p>Cada procesador (Pi) y cada módulo de memoria (Mi) tienen su
propio bus. Existe un conmutador (S) en los puntos de intersección
que permite conectar un bus de memoria con un bus de procesador.
Para evitar conflictos cuando más de un procesador pretende acceder
al mismo módulo de memoria se establece un orden de prioridad. Se
trata de una red sin bloqueo con una conectividad completa pero de
alta complejidad.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u78.jpg"></center>

</table></center>	

<center><h1><font color = "red" >*Tema: Aplicaciones.*</font></h1></center>
<center><table border = "3"><td>
<center><h2><font color = "white">
<p>- Sistemas de memoria distribuida (multicomputadores).</p>
<p>Cada procesador tiene su propia memoria y la comunicación se
realiza por intercambio explícito de mensajes a través de una red.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u79.jpg"></center>

<center><h2><font color = "white">
<p>- Ventajas</p>
<p>* El número de nodos puede ir desde algunas decenas hasta
varios miles (o más).</p>
<p>* La arquitectura de paso de mensajes tiene ventajas sobre la de
memoria compartida cuando el número de procesadores es
grande.</p>
<p>* El número de canales físicos entre nodos suele oscilar entre
cuatro y ocho.</p>
<p>*Esta arquitectura es directamente escalable y presenta un bajo
coste para sistemas grandes.</p>
<p>* Un problema se especifica como un conjunto de procesos que
se comunican entre sí y que se hacen corresponder sobre la
estructura física de procesadores.</p>

<p>- Desventajas.</p>
<p>* Se necesitan técnicas de sincronización para acceder a las
variables compartidas.</p>
<p>* La contención en la memoria puede reducir significativamente
la velocidad.</p>
<p>* No son fácilmente escalables a un gran número de
procesadores.</p>
<p>- Redes de interconexión estáticas.</p>
<p>Los multicomputadores utilizan redes estáticas con enlaces directos
entre nodos. Cuando un nodo recibe un mensaje lo procesa si viene
dirigido a dicho nodo. Si el mensaje no va dirigido al nodo receptor
lo reenvía a otro por alguno de sus enlaces de salida siguiendo un
protocolo de encaminamiento.</p>
</font></h2></center>
<center><img src = "PaginaWeb/imagenes/u80.jpg"></center>

<center><h2><font color = "white">
<p>- Propiedades más significativas.</p>
<p>* Topología de la red: determina el patrón de interconexión
entre nodos.</p>
<p>* Diámetro de la red: distancia máxima de los caminos más
cortos entre dos nodos de la red.</p>
<p>* Latencia: retardo de tiempo en el peor caso para un mensaje
transferido a través de la red.</p>
<p>* Ancho de banda: Transferencia máxima de datos en
Mbytes/segundo.</p>
<p>* Escalabilidad: posibilidad de expansión modular de la red.
<p>* Grado de un nodo: número de enlaces o canales que inciden
en el nodo.</p>
<p>* Algoritmo de encaminamiento: determina el camino que
debe seguir un mensaje desde el nodo emisor al nodo receptor.</p>
<p>- Casos para estudio</p>
<p>Por numerosos motivos, el procesamiento distribuido se ha
convertido en un área de gran importancia e interés dentro de la
ciencia de la computación, produciendo profundas transformaciones
en las líneas de investigación y desarrollo.</p>
<p>Interesa realizar investigación en la especificación, transformación,
optimización y evaluación de algoritmos distribuidos y paralelos.
Esto incluye el diseño y desarrollo de sistemas paralelos, la
transformación de algoritmos secuenciales en paralelos, y las
métricas de evaluación de performance sobre distintas plataformas
de soporte (hardware y software). Más allá de las mejoras constantes
en las arquitecturas físicas de soporte, uno de los mayores desafíos
se centra en cómo aprovechar al máximo la potencia de las mismas.</p>

<p>- Líneas de investigación y desarrollo</p>
<p>* Paralelización de algoritmos secuenciales. Diseño y
<p>optimización de algoritmos.</p>
<p>* Arquitecturas multicore y multithreading en multicore.</p>
<p>* Modelos de representación y predicción de performance de
algoritmos paralelos.</p>
<p>* Mapping y scheduling de aplicaciones paralelas sobre distintas
arquitecturas multiprocesador.</p>
<p>* Métricas del paralelismo. Speedup, eficiencia, rendimiento,
granularidad, superlinealidad.</p>
<p>* Balance de carga estático y dinámico. Técnicas de balanceo de
carga.</p>
<p>* Análisis de los problemas de migración y asignación óptima de
procesos y datos a procesadores.</p>
<p>* Patrones de diseño de algoritmos paralelos.</p>
<p>* Escalabilidad de algoritmos paralelos en arquitecturas
multiprocesador distribuidas.</p>
<p>* Implementación de soluciones sobre diferentes modelos de
arquitectura homogéneas y heterogéneas.</p>
<p>* Laboratorios remotos para el acceso transparente a recursos de
cómputo paralelo.</p>
</font></h2></center>

</table></center>		

<center><a href = "PaginaW.html"><font color = "white">Inicio</a> -
<a href = "uni1.html"><font color = "white">Unidad 1</a> -
<a href = "uni2.html"><font color = "white">Unidad 2</a> -
<a href = "uni3.html"><font color = "white">Unidad 3</a> -
<a href = "uni4.html"><font color = "white">Unidad 4</font></a> <br><font color = "white"> Creado por Arnoldo Gaona Hernandez, Ingenieria
en Sistemas Computacionales Tec Campus Saltillo</font>.
</center>
</body>
</html>